{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Catch-Up AI Training with a Q-table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import all the necessary python libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define all the function that will be use to simulate a game of Catch-Up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gameOver(state):\n",
    "    '''\n",
    "    gameOver() returns True if there are no more pieces to be chosen.\n",
    "    '''\n",
    "    piecesLeft = (sum(state[:n]))\n",
    "    return not piecesLeft\n",
    "\n",
    "def posAction(state):\n",
    "    '''\n",
    "    posAction(state) will given a state returns a list of all the pieces\n",
    "    that could be chosen.\n",
    "    '''\n",
    "    return [x+1 for x in range(n) if state[x] == 1]\n",
    "\n",
    "def nextState(state, action):\n",
    "    '''\n",
    "    nextState(state, action) transforms one state of the games into\n",
    "    the next one given the taken action.\n",
    "    '''\n",
    "    state = list(state)\n",
    "    state[action - 1] = 0\n",
    "    state[n] += action\n",
    "    if state[n] >= state[n+1]:\n",
    "        state = switchPlayer(state)\n",
    "    return tuple(state)\n",
    "\n",
    "def switchPlayer(state):\n",
    "    '''\n",
    "    switchPlayer(state) transforms the state as to make sure that the \n",
    "    player with the lower score is on play.\n",
    "    '''\n",
    "    state = list(state)\n",
    "    state[n], state[n+1] = state[n+1], state[n]\n",
    "    return tuple(state)\n",
    "\n",
    "def determineReward(state, action):\n",
    "    '''\n",
    "    determineReward(state) returns a reward if the player wins or loses\n",
    "    a game of catch-up.\n",
    "    '''\n",
    "    newState = nextState(state, action)\n",
    "    if gameOver(newState) and state[n] + action > state[n+1]:\n",
    "        return 1\n",
    "    elif gameOver(newState) and state[n] + action < state[n+1]:\n",
    "        return -1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "def buildStateSpace(state, first = True):\n",
    "    '''\n",
    "    buildStateSpace(state, first = True) will return a generator that\n",
    "    runs over all the possible game states. Take in mind that \n",
    "    duplicates will be included in this iteration.\n",
    "    '''\n",
    "    if first: yield state\n",
    "    posAct = posAction(state)\n",
    "    if len(posAct) > 0:\n",
    "        for action in posAct:\n",
    "            newState = nextState(state, action)\n",
    "            yield newState\n",
    "            for x in buildStateSpace(newState, False):\n",
    "                yield x\n",
    "    else:\n",
    "        yield state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q-learning formula:\\\n",
    "For a given state $s$ and action $a$:\n",
    "$Q^{new}(s_t, a_t)\\leftarrow (1 - \\alpha)\\cdot Q(s_t, a_t) + \\alpha\\cdot\\left(r_t + \\gamma\\cdot \\max_a Q(s_{t+1}, a)\\right)$\\\n",
    "with $\\alpha$ the learning rate and $\\gamma$ the discount rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class qTable(object):\n",
    "    '''\n",
    "    qTable() is a class object that contains the Q table and\n",
    "    is able to do operations with this table.\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, n, learningRate, discountRate):\n",
    "        '''\n",
    "        Initializes the qTable object.\n",
    "        '''\n",
    "        self.lr = learningRate\n",
    "        self.dr = discountRate        \n",
    "        self.table = {}\n",
    "        self.trans = {}\n",
    "        combTable = []\n",
    "            \n",
    "        state = tuple([1 if i < n else 0 for i in range(n+2)])\n",
    "        print(\"Building the Q table...\")\n",
    "        for piece in buildStateSpace(state, True):\n",
    "            self.table[piece] = [0 for _ in range(n)]\n",
    "            # (np.random.random()-0.5)*2\n",
    "            \n",
    "    def update(self, state, newState, action, reward):\n",
    "        '''\n",
    "        update(self, state, newState, action, reward, lr, dr) updates\n",
    "        the Q table given the information gained from the reward, next and\n",
    "        current state. Both learning and discount rate of the model are\n",
    "        required inputs as well.\n",
    "        '''\n",
    "        if state[n] + action >= state[n+1]: factor = -1\n",
    "        else: factor = 1\n",
    "        \n",
    "        self.table[state][action-1] = (1 - self.lr) * self.table[state][action-1] + \\\n",
    "        self.lr * (reward + factor * self.dr * max(self.table[newState]))\n",
    "        \n",
    "    def translate(self):\n",
    "        '''\n",
    "        translate() translates the values in the Q table into a table\n",
    "        that gives the suggest move for each state.\n",
    "        '''\n",
    "        for x in self.table.keys():\n",
    "            posAct = posAction(x)\n",
    "            posQ = self.table[x].copy()\n",
    "            action = 0\n",
    "            while len(posAct) > 0:\n",
    "                action = posQ.index(max(posQ)) + 1\n",
    "                if action in posAct:\n",
    "                    break\n",
    "                else:\n",
    "                    posQ[action - 1] = min(posQ) - 1\n",
    "            self.trans[x] = action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def botRandom(state):\n",
    "    '''\n",
    "    Bot chooses any move at random.\n",
    "    '''\n",
    "    posAct = posAction(state)\n",
    "    diff = state[n+1] - state[n]\n",
    "    combAct = []\n",
    "    while (diff > 0 or len(combAct) == 0) and not len(posAct) == 0:\n",
    "        action = np.random.choice(posAct)\n",
    "        combAct.append(action)\n",
    "        posAct.remove(action)\n",
    "        diff = state[n+1] - state[n] - sum(combAct)\n",
    "        \n",
    "    return combAct\n",
    "\n",
    "def botMaxScore(state):\n",
    "    '''\n",
    "    Bot maximizes its scores on every turn, \n",
    "    extending their leads by as much as possible.\n",
    "    '''\n",
    "    posAct = posAction(state)\n",
    "    diff = state[n+1] - state[n]\n",
    "    combAct = []\n",
    "    \n",
    "    # Add the biggest possible pieces that don't trigger\n",
    "    # a change in player.\n",
    "    while not len([x for x in posAct if x < diff]) <= 1 and min(posAct) < diff:\n",
    "        action = max([x for x in posAct if x < diff])\n",
    "        combAct.append(action)\n",
    "        posAct.remove(action)\n",
    "        diff = state[n+1] - state[n] - sum(combAct)\n",
    "    \n",
    "    # Add the biggest possible piece to create the biggest\n",
    "    # extension.\n",
    "    combAct.append(max(posAct))\n",
    "\n",
    "    return combAct\n",
    "\n",
    "def botMinScore(state):\n",
    "    '''\n",
    "    Bot minimizes its scores on every turn\n",
    "    keeping their scores as close as possible.\n",
    "    '''\n",
    "    posAct = posAction(state)\n",
    "    diff = state[n+1] - state[n]\n",
    "    combAct = []\n",
    "    \n",
    "    # Add the biggest possible pieces that don't trigger\n",
    "    # a change in player.\n",
    "    while not len([x for x in posAct if x < diff]) <= 1 and min(posAct) < diff:\n",
    "        action = max([x for x in posAct if x < diff])\n",
    "        combAct.append(action)\n",
    "        posAct.remove(action)\n",
    "        diff = state[n+1] - state[n] - sum(combAct)\n",
    "    \n",
    "    # Add the smallest possible piece to create the smallest\n",
    "    # extensions but not equal.\n",
    "    combAct.append(min(posAct))\n",
    "    \n",
    "    return combAct\n",
    "\n",
    "def botUseMostNums(state):\n",
    "    '''\n",
    "    Bot uses as many numbers as possible, reducing\n",
    "    the numbers available for the opponent.   \n",
    "    '''\n",
    "    posAct = posAction(state)\n",
    "    diff = state[n+1] - state[n]\n",
    "    combAct = []\n",
    "    \n",
    "    while not len([x for x in posAct if x < diff]) <= 1 and min(posAct) < diff:\n",
    "        action = min(posAct)\n",
    "        combAct.append(action)\n",
    "        posAct.remove(action)\n",
    "        diff = state[n+1] - state[n] - sum(combAct)\n",
    "        \n",
    "    # Add the biggest possible piece to create the biggest\n",
    "    # extension.\n",
    "    combAct.append(max(posAct))\n",
    "\n",
    "    return combAct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def playerAI(state, qTable):\n",
    "    '''\n",
    "    The AI will look into the Q table for the action with\n",
    "    the highest future reward and pick that action. This\n",
    "    function recurses until the AI's turn lapses.\n",
    "    '''\n",
    "    posAct = posAction(state)\n",
    "    diff = state[n+1] - state[n]\n",
    "    \n",
    "    posQ = qTable.table[state].copy()\n",
    "    for _ in range(len(posQ)):\n",
    "        action = posQ.index(max(posQ)) + 1\n",
    "        if action in posAct:\n",
    "            yield action\n",
    "            if diff - action > 0:\n",
    "                newState = nextState(state, action)\n",
    "                yield from playerAI(newState, qTable)\n",
    "            break\n",
    "        else:\n",
    "            posQ[action - 1] = min(posQ) - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let the AI learn by playing games against the random bot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building the Q table...\n",
      "Training the model...\n",
      "Episode  0  completed.\n",
      "Episode  50  completed.\n",
      "Episode  100  completed.\n",
      "Episode  150  completed.\n",
      "Episode  200  completed.\n",
      "Episode  250  completed.\n",
      "Episode  300  completed.\n",
      "Episode  350  completed.\n",
      "Episode  400  completed.\n",
      "Episode  450  completed.\n",
      "Episode  500  completed.\n",
      "Episode  550  completed.\n",
      "Episode  600  completed.\n",
      "Episode  650  completed.\n",
      "Episode  700  completed.\n",
      "Episode  750  completed.\n",
      "Episode  800  completed.\n",
      "Episode  850  completed.\n",
      "Episode  900  completed.\n",
      "Episode  950  completed.\n",
      "Done training the model!\n"
     ]
    }
   ],
   "source": [
    "learningRate = 0.1\n",
    "discountRate = 0.9\n",
    "n = 4\n",
    "reward = 10\n",
    "\n",
    "# initialize the Q table\n",
    "qTab = qTable(n, learningRate, discountRate)\n",
    "\n",
    "print(\"Training the model...\")\n",
    "for _ in range(1000):\n",
    "    # assemble starting state\n",
    "    state = tuple([1 if i < n else 0 for i in range(n+2)])\n",
    "    # while the game is not finished\n",
    "    while not gameOver(state):\n",
    "        # Player One (Bot)\n",
    "        actions = botRandom(state)\n",
    "\n",
    "        for action in actions:\n",
    "            # find the next state corresponding to the action selected\n",
    "            newState = nextState(state, action)\n",
    "            # get reward factor\n",
    "            reward = determineReward(state, action)\n",
    "            # update Q table\n",
    "            qTab.update(state, newState, action, reward)\n",
    "            # got to new state\n",
    "            state = newState      \n",
    "            \n",
    "    if _%50 == 0: print(\"Episode \", _, \" completed.\")\n",
    "print(\"Done training the model!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(1, 1, 1, 1, 0, 0): [0.0, 0.0, 0.0, -0.7279589700605122],\n",
       " (0, 1, 1, 1, 0, 1): [0, 0.0, 0.0, -0.809705738458582],\n",
       " (0, 0, 1, 1, 1, 2): [0, 0, -0.8980555769452977, 0.0],\n",
       " (0, 0, 0, 1, 2, 4): [0, 0, 0, 0.9997815254994715],\n",
       " (0, 0, 0, 0, 4, 6): [0, 0, 0, 0],\n",
       " (0, 0, 1, 0, 2, 5): [0, 0, 0.0, 0],\n",
       " (0, 0, 0, 0, 5, 5): [0, 0, 0, 0],\n",
       " (0, 1, 0, 1, 1, 3): [0, -0.8989742887107122, 0, 0.0],\n",
       " (0, 0, 0, 1, 3, 3): [0, 0, 0, 0.9999999272502554],\n",
       " (0, 0, 0, 0, 3, 7): [0, 0, 0, 0],\n",
       " (0, 1, 0, 0, 3, 5): [0, 0.0, 0, 0],\n",
       " (0, 1, 1, 0, 1, 4): [0, 0.8999731116393284, -0.8994301440258012, 0],\n",
       " (0, 0, 1, 0, 3, 4): [0, 0, 0.9999999985249581, 0],\n",
       " (0, 1, 0, 0, 4, 4): [0, 0.9999999191669505, 0, 0],\n",
       " (1, 0, 1, 1, 0, 2): [0.0, 0, 0.0, -0.8069685859612457],\n",
       " (1, 0, 0, 1, 2, 3): [-0.8997541480886676, 0, 0, 0.0],\n",
       " (1, 0, 0, 0, 3, 6): [-0.9998033729495244, 0, 0, 0],\n",
       " (1, 0, 1, 0, 2, 4): [0.8998094357546051, 0, 0.0, 0],\n",
       " (1, 0, 0, 0, 4, 5): [0.0, 0, 0, 0],\n",
       " (1, 1, 0, 1, 0, 3): [0.0, 0.0, 0, 0.0],\n",
       " (1, 1, 0, 0, 3, 4): [-0.8995120207107371, 0.0, 0, 0],\n",
       " (1, 1, 1, 0, 0, 4): [0.8087539027237466, 0.8092293418425133, 0.0, 0]}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qTab.table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(1, 1, 1, 1, 0, 0): 1,\n",
       " (0, 1, 1, 1, 0, 1): 2,\n",
       " (0, 0, 1, 1, 1, 2): 4,\n",
       " (0, 0, 0, 1, 2, 4): 4,\n",
       " (0, 0, 0, 0, 4, 6): 0,\n",
       " (0, 0, 1, 0, 2, 5): 3,\n",
       " (0, 0, 0, 0, 5, 5): 0,\n",
       " (0, 1, 0, 1, 1, 3): 4,\n",
       " (0, 0, 0, 1, 3, 3): 4,\n",
       " (0, 0, 0, 0, 3, 7): 0,\n",
       " (0, 1, 0, 0, 3, 5): 2,\n",
       " (0, 1, 1, 0, 1, 4): 2,\n",
       " (0, 0, 1, 0, 3, 4): 3,\n",
       " (0, 1, 0, 0, 4, 4): 2,\n",
       " (1, 0, 1, 1, 0, 2): 1,\n",
       " (1, 0, 0, 1, 2, 3): 4,\n",
       " (1, 0, 0, 0, 3, 6): 1,\n",
       " (1, 0, 1, 0, 2, 4): 1,\n",
       " (1, 0, 0, 0, 4, 5): 1,\n",
       " (1, 1, 0, 1, 0, 3): 1,\n",
       " (1, 1, 0, 0, 3, 4): 2,\n",
       " (1, 1, 1, 0, 0, 4): 2}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qTab.translate()\n",
    "qTab.trans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The AI has now learned how to play the game and we are now checking how well it does against a random opponent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Playing games...\n",
      "Game  0 finished.\n",
      "Game  1000 finished.\n",
      "Game  2000 finished.\n",
      "Game  3000 finished.\n",
      "Game  4000 finished.\n",
      "Game  5000 finished.\n",
      "Game  6000 finished.\n",
      "Game  7000 finished.\n",
      "Game  8000 finished.\n",
      "Game  9000 finished.\n",
      "10000\n",
      "0.3119\n"
     ]
    }
   ],
   "source": [
    "score = []\n",
    "\n",
    "print(\"Playing games...\")\n",
    "for _ in range(10000):\n",
    "    # assemble starting state\n",
    "    state = tuple([1 if i < n else 0 for i in range(n+2)])\n",
    "    # while the game is not finished\n",
    "    while not gameOver(state):\n",
    "        # Player One\n",
    "        actions = botRandom(state)\n",
    "        #actions = botMaxScore(state)\n",
    "        #actions = botMinScore(state)\n",
    "        #actions = botUseMostNums(state)\n",
    "        #actions = list(playerAI(state, qTab))\n",
    "        \n",
    "        if len(actions) == len(posAction(state)):\n",
    "            if state[n] + sum(actions) > state[n+1]: score.append(1) # AI loses\n",
    "            elif state[n] + sum(actions) < state[n+1]: score.append(0) # AI wins\n",
    "            else: score.append(0.5) # draw\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "        for action in actions:\n",
    "            #print(\"BOT: \", state)\n",
    "            #print(\"-->\", action)\n",
    "            state = nextState(state, action)\n",
    "            #print(\"BOT: \", state)\n",
    "        \n",
    "        # Player Two\n",
    "        if not gameOver(state):\n",
    "            #actions = botRandom(state)\n",
    "            #actions = botMaxScore(state)\n",
    "            #actions = botMinScore(state)\n",
    "            #actions = botUseMostNums(state)\n",
    "            actions = list(playerAI(state, qTab))  \n",
    "            \n",
    "            if len(actions) == len(posAction(state)):\n",
    "                if state[n] + sum(actions) > state[n+1]: score.append(0) # AI wins\n",
    "                elif state[n] + sum(actions) < state[n+1]: score.append(1) # AI loses\n",
    "                else: score.append(0.5) # draw\n",
    "            else:\n",
    "                pass\n",
    "            \n",
    "            for action in actions:\n",
    "                #print(\"AI: \", state)\n",
    "                #print(\"-->\", action)\n",
    "                state = nextState(state, action)\n",
    "                #print(\"AI: \", state)\n",
    "    \n",
    "    if _%1000==0: print(\"Game \", _, \"finished.\")\n",
    "    #print(score)\n",
    "    #if score[_] == 1: print(\"AI wins!\")\n",
    "    #elif score[_] == 0: print(\"BOT wins!\")\n",
    "    #else: print(\"Draw!\")\n",
    "\n",
    "print(len(score))\n",
    "print(np.mean(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
